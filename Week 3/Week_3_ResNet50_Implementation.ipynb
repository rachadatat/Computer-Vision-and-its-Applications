{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv68lqztSCox"
      },
      "source": [
        "We will start by setting up a Google Colab environment to work with files stored in Google Drive. First, we mount the Google Drive so that we can access its contents within the Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx79ckA3BROE",
        "outputId": "5f7d2e0e-3da2-4f25-9bcd-92e44ad22625"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR_U3-nuBZJC",
        "outputId": "a8c1464a-f580-499e-d157-5a2b1c639fc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                          Version\n",
            "-------------------------------- -------------------\n",
            "absl-py                          1.4.0\n",
            "accelerate                       0.34.2\n",
            "aiohappyeyeballs                 2.4.0\n",
            "aiohttp                          3.10.5\n",
            "aiosignal                        1.3.1\n",
            "alabaster                        0.7.16\n",
            "albucore                         0.0.16\n",
            "albumentations                   1.4.15\n",
            "altair                           4.2.2\n",
            "annotated-types                  0.7.0\n",
            "anyio                            3.7.1\n",
            "argon2-cffi                      23.1.0\n",
            "argon2-cffi-bindings             21.2.0\n",
            "array_record                     0.5.1\n",
            "arviz                            0.19.0\n",
            "astropy                          6.1.3\n",
            "astropy-iers-data                0.2024.9.16.0.32.21\n",
            "astunparse                       1.6.3\n",
            "async-timeout                    4.0.3\n",
            "atpublic                         4.1.0\n",
            "attrs                            24.2.0\n",
            "audioread                        3.0.1\n",
            "autograd                         1.7.0\n",
            "babel                            2.16.0\n",
            "backcall                         0.2.0\n",
            "beautifulsoup4                   4.12.3\n",
            "bigframes                        1.18.0\n",
            "bigquery-magics                  0.2.0\n",
            "bleach                           6.1.0\n",
            "blinker                          1.4\n",
            "blis                             0.7.11\n",
            "blosc2                           2.0.0\n",
            "bokeh                            3.4.3\n",
            "bqplot                           0.12.43\n",
            "branca                           0.7.2\n",
            "build                            1.2.2\n",
            "CacheControl                     0.14.0\n",
            "cachetools                       5.5.0\n",
            "catalogue                        2.0.10\n",
            "certifi                          2024.8.30\n",
            "cffi                             1.17.1\n",
            "chardet                          5.2.0\n",
            "charset-normalizer               3.3.2\n",
            "chex                             0.1.86\n",
            "clarabel                         0.9.0\n",
            "click                            8.1.7\n",
            "cloudpathlib                     0.19.0\n",
            "cloudpickle                      2.2.1\n",
            "cmake                            3.30.3\n",
            "cmdstanpy                        1.2.4\n",
            "colorcet                         3.1.0\n",
            "colorlover                       0.3.0\n",
            "colour                           0.1.5\n",
            "community                        1.0.0b1\n",
            "confection                       0.1.5\n",
            "cons                             0.4.6\n",
            "contextlib2                      21.6.0\n",
            "contourpy                        1.3.0\n",
            "cryptography                     43.0.1\n",
            "cuda-python                      12.2.1\n",
            "cudf-cu12                        24.4.1\n",
            "cufflinks                        0.17.3\n",
            "cupy-cuda12x                     12.2.0\n",
            "cvxopt                           1.3.2\n",
            "cvxpy                            1.5.3\n",
            "cycler                           0.12.1\n",
            "cymem                            2.0.8\n",
            "Cython                           3.0.11\n",
            "dask                             2024.8.0\n",
            "datascience                      0.17.6\n",
            "db-dtypes                        1.3.0\n",
            "dbus-python                      1.2.18\n",
            "debugpy                          1.6.6\n",
            "decorator                        4.4.2\n",
            "defusedxml                       0.7.1\n",
            "distributed                      2024.8.0\n",
            "distro                           1.7.0\n",
            "dlib                             19.24.2\n",
            "dm-tree                          0.1.8\n",
            "docstring_parser                 0.16\n",
            "docutils                         0.18.1\n",
            "dopamine_rl                      4.0.9\n",
            "duckdb                           1.1.0\n",
            "earthengine-api                  1.0.0\n",
            "easydict                         1.13\n",
            "ecos                             2.0.14\n",
            "editdistance                     0.8.1\n",
            "eerepr                           0.0.4\n",
            "einops                           0.8.0\n",
            "en-core-web-sm                   3.7.1\n",
            "entrypoints                      0.4\n",
            "et-xmlfile                       1.1.0\n",
            "etils                            1.9.4\n",
            "etuples                          0.3.9\n",
            "eval_type_backport               0.2.0\n",
            "exceptiongroup                   1.2.2\n",
            "fastai                           2.7.17\n",
            "fastcore                         1.7.8\n",
            "fastdownload                     0.0.7\n",
            "fastjsonschema                   2.20.0\n",
            "fastprogress                     1.0.3\n",
            "fastrlock                        0.8.2\n",
            "filelock                         3.16.1\n",
            "firebase-admin                   6.5.0\n",
            "Flask                            2.2.5\n",
            "flatbuffers                      24.3.25\n",
            "flax                             0.8.5\n",
            "folium                           0.17.0\n",
            "fonttools                        4.53.1\n",
            "frozendict                       2.4.4\n",
            "frozenlist                       1.4.1\n",
            "fsspec                           2024.6.1\n",
            "future                           1.0.0\n",
            "gast                             0.6.0\n",
            "gcsfs                            2024.6.1\n",
            "GDAL                             3.6.4\n",
            "gdown                            5.2.0\n",
            "geemap                           0.34.3\n",
            "gensim                           4.3.3\n",
            "geocoder                         1.38.1\n",
            "geographiclib                    2.0\n",
            "geopandas                        1.0.1\n",
            "geopy                            2.4.1\n",
            "gin-config                       0.5.0\n",
            "glob2                            0.7\n",
            "google                           2.0.3\n",
            "google-ai-generativelanguage     0.6.6\n",
            "google-api-core                  2.19.2\n",
            "google-api-python-client         2.137.0\n",
            "google-auth                      2.27.0\n",
            "google-auth-httplib2             0.2.0\n",
            "google-auth-oauthlib             1.2.1\n",
            "google-cloud-aiplatform          1.67.1\n",
            "google-cloud-bigquery            3.25.0\n",
            "google-cloud-bigquery-connection 1.15.5\n",
            "google-cloud-bigquery-storage    2.26.0\n",
            "google-cloud-bigtable            2.26.0\n",
            "google-cloud-core                2.4.1\n",
            "google-cloud-datastore           2.19.0\n",
            "google-cloud-firestore           2.16.1\n",
            "google-cloud-functions           1.16.5\n",
            "google-cloud-iam                 2.15.2\n",
            "google-cloud-language            2.13.4\n",
            "google-cloud-pubsub              2.23.1\n",
            "google-cloud-resource-manager    1.12.5\n",
            "google-cloud-storage             2.8.0\n",
            "google-cloud-translate           3.15.5\n",
            "google-colab                     1.0.0\n",
            "google-crc32c                    1.6.0\n",
            "google-generativeai              0.7.2\n",
            "google-pasta                     0.2.0\n",
            "google-resumable-media           2.7.2\n",
            "googleapis-common-protos         1.65.0\n",
            "googledrivedownloader            0.4\n",
            "graphviz                         0.20.3\n",
            "greenlet                         3.1.1\n",
            "grpc-google-iam-v1               0.13.1\n",
            "grpcio                           1.64.1\n",
            "grpcio-status                    1.48.2\n",
            "gspread                          6.0.2\n",
            "gspread-dataframe                3.3.1\n",
            "gym                              0.25.2\n",
            "gym-notices                      0.0.8\n",
            "h5netcdf                         1.3.0\n",
            "h5py                             3.11.0\n",
            "holidays                         0.57\n",
            "holoviews                        1.19.1\n",
            "html5lib                         1.1\n",
            "httpimport                       1.4.0\n",
            "httplib2                         0.22.0\n",
            "huggingface-hub                  0.24.7\n",
            "humanize                         4.10.0\n",
            "hyperopt                         0.2.7\n",
            "ibis-framework                   9.2.0\n",
            "idna                             3.10\n",
            "imageio                          2.35.1\n",
            "imageio-ffmpeg                   0.5.1\n",
            "imagesize                        1.4.1\n",
            "imbalanced-learn                 0.12.3\n",
            "imgaug                           0.4.0\n",
            "immutabledict                    4.2.0\n",
            "importlib_metadata               8.5.0\n",
            "importlib_resources              6.4.5\n",
            "imutils                          0.5.4\n",
            "inflect                          7.4.0\n",
            "iniconfig                        2.0.0\n",
            "intel-cmplr-lib-ur               2024.2.1\n",
            "intel-openmp                     2024.2.1\n",
            "ipyevents                        2.0.2\n",
            "ipyfilechooser                   0.6.0\n",
            "ipykernel                        5.5.6\n",
            "ipyleaflet                       0.19.2\n",
            "ipyparallel                      8.8.0\n",
            "ipython                          7.34.0\n",
            "ipython-genutils                 0.2.0\n",
            "ipython-sql                      0.5.0\n",
            "ipytree                          0.2.2\n",
            "ipywidgets                       7.7.1\n",
            "itsdangerous                     2.2.0\n",
            "jax                              0.4.33\n",
            "jax-cuda12-pjrt                  0.4.33\n",
            "jax-cuda12-plugin                0.4.33\n",
            "jaxlib                           0.4.33\n",
            "jeepney                          0.7.1\n",
            "jellyfish                        1.1.0\n",
            "jieba                            0.42.1\n",
            "Jinja2                           3.1.4\n",
            "joblib                           1.4.2\n",
            "jsonpickle                       3.3.0\n",
            "jsonschema                       4.23.0\n",
            "jsonschema-specifications        2023.12.1\n",
            "jupyter-client                   6.1.12\n",
            "jupyter-console                  6.1.0\n",
            "jupyter_core                     5.7.2\n",
            "jupyter-leaflet                  0.19.2\n",
            "jupyter-server                   1.24.0\n",
            "jupyterlab_pygments              0.3.0\n",
            "jupyterlab_widgets               3.0.13\n",
            "kaggle                           1.6.17\n",
            "kagglehub                        0.3.0\n",
            "keras                            3.4.1\n",
            "keyring                          23.5.0\n",
            "kiwisolver                       1.4.7\n",
            "langcodes                        3.4.0\n",
            "language_data                    1.2.0\n",
            "launchpadlib                     1.10.16\n",
            "lazr.restfulclient               0.14.4\n",
            "lazr.uri                         1.0.6\n",
            "lazy_loader                      0.4\n",
            "libclang                         18.1.1\n",
            "librosa                          0.10.2.post1\n",
            "lightgbm                         4.5.0\n",
            "linkify-it-py                    2.0.3\n",
            "llvmlite                         0.43.0\n",
            "locket                           1.0.0\n",
            "logical-unification              0.4.6\n",
            "lxml                             4.9.4\n",
            "marisa-trie                      1.2.0\n",
            "Markdown                         3.7\n",
            "markdown-it-py                   3.0.0\n",
            "MarkupSafe                       2.1.5\n",
            "matplotlib                       3.7.1\n",
            "matplotlib-inline                0.1.7\n",
            "matplotlib-venn                  1.1.1\n",
            "mdit-py-plugins                  0.4.2\n",
            "mdurl                            0.1.2\n",
            "miniKanren                       1.0.3\n",
            "missingno                        0.5.2\n",
            "mistune                          0.8.4\n",
            "mizani                           0.11.4\n",
            "mkl                              2024.2.2\n",
            "ml-dtypes                        0.4.1\n",
            "mlxtend                          0.23.1\n",
            "more-itertools                   10.5.0\n",
            "moviepy                          1.0.3\n",
            "mpmath                           1.3.0\n",
            "msgpack                          1.0.8\n",
            "multidict                        6.1.0\n",
            "multipledispatch                 1.0.0\n",
            "multitasking                     0.0.11\n",
            "murmurhash                       1.0.10\n",
            "music21                          9.1.0\n",
            "namex                            0.0.8\n",
            "natsort                          8.4.0\n",
            "nbclassic                        1.1.0\n",
            "nbclient                         0.10.0\n",
            "nbconvert                        6.5.4\n",
            "nbformat                         5.10.4\n",
            "nest-asyncio                     1.6.0\n",
            "networkx                         3.3\n",
            "nibabel                          5.2.1\n",
            "nltk                             3.8.1\n",
            "notebook                         6.5.5\n",
            "notebook_shim                    0.2.4\n",
            "numba                            0.60.0\n",
            "numexpr                          2.10.1\n",
            "numpy                            1.26.4\n",
            "nvidia-cublas-cu12               12.6.1.4\n",
            "nvidia-cuda-cupti-cu12           12.6.68\n",
            "nvidia-cuda-nvcc-cu12            12.6.68\n",
            "nvidia-cuda-runtime-cu12         12.6.68\n",
            "nvidia-cudnn-cu12                9.4.0.58\n",
            "nvidia-cufft-cu12                11.2.6.59\n",
            "nvidia-cusolver-cu12             11.6.4.69\n",
            "nvidia-cusparse-cu12             12.5.3.3\n",
            "nvidia-nccl-cu12                 2.23.4\n",
            "nvidia-nvjitlink-cu12            12.6.68\n",
            "nvtx                             0.2.10\n",
            "oauth2client                     4.1.3\n",
            "oauthlib                         3.2.2\n",
            "opencv-contrib-python            4.10.0.84\n",
            "opencv-python                    4.10.0.84\n",
            "opencv-python-headless           4.10.0.84\n",
            "openpyxl                         3.1.5\n",
            "opt-einsum                       3.3.0\n",
            "optax                            0.2.3\n",
            "optree                           0.12.1\n",
            "orbax-checkpoint                 0.6.4\n",
            "osqp                             0.6.7.post0\n",
            "packaging                        24.1\n",
            "pandas                           2.1.4\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.23.1\n",
            "pandas-stubs                     2.1.4.231227\n",
            "pandocfilters                    1.5.1\n",
            "panel                            1.4.5\n",
            "param                            2.1.1\n",
            "parso                            0.8.4\n",
            "parsy                            2.1\n",
            "partd                            1.4.2\n",
            "pathlib                          1.0.1\n",
            "patsy                            0.5.6\n",
            "peewee                           3.17.6\n",
            "pexpect                          4.9.0\n",
            "pickleshare                      0.7.5\n",
            "pillow                           10.4.0\n",
            "pip                              24.1.2\n",
            "pip-tools                        7.4.1\n",
            "platformdirs                     4.3.6\n",
            "plotly                           5.24.1\n",
            "plotnine                         0.13.6\n",
            "pluggy                           1.5.0\n",
            "polars                           1.6.0\n",
            "pooch                            1.8.2\n",
            "portpicker                       1.5.2\n",
            "prefetch_generator               1.0.3\n",
            "preshed                          3.0.9\n",
            "prettytable                      3.11.0\n",
            "proglog                          0.1.10\n",
            "progressbar2                     4.5.0\n",
            "prometheus_client                0.21.0\n",
            "promise                          2.3\n",
            "prompt_toolkit                   3.0.47\n",
            "prophet                          1.1.5\n",
            "proto-plus                       1.24.0\n",
            "protobuf                         3.20.3\n",
            "psutil                           5.9.5\n",
            "psycopg2                         2.9.9\n",
            "ptyprocess                       0.7.0\n",
            "py-cpuinfo                       9.0.0\n",
            "py4j                             0.10.9.7\n",
            "pyarrow                          14.0.2\n",
            "pyarrow-hotfix                   0.6\n",
            "pyasn1                           0.6.1\n",
            "pyasn1_modules                   0.4.1\n",
            "pycocotools                      2.0.8\n",
            "pycparser                        2.22\n",
            "pydantic                         2.9.2\n",
            "pydantic_core                    2.23.4\n",
            "pydata-google-auth               1.8.2\n",
            "pydot                            3.0.1\n",
            "pydot-ng                         2.0.0\n",
            "pydotplus                        2.0.2\n",
            "PyDrive                          1.3.1\n",
            "PyDrive2                         1.20.0\n",
            "pyerfa                           2.0.1.4\n",
            "pygame                           2.6.0\n",
            "Pygments                         2.18.0\n",
            "PyGObject                        3.42.1\n",
            "PyJWT                            2.9.0\n",
            "pymc                             5.16.2\n",
            "pymystem3                        0.2.0\n",
            "pynvjitlink-cu12                 0.3.0\n",
            "pyogrio                          0.9.0\n",
            "PyOpenGL                         3.1.7\n",
            "pyOpenSSL                        24.2.1\n",
            "pyparsing                        3.1.4\n",
            "pyperclip                        1.9.0\n",
            "pyproj                           3.6.1\n",
            "pyproject_hooks                  1.1.0\n",
            "pyshp                            2.3.1\n",
            "PySocks                          1.7.1\n",
            "pytensor                         2.25.4\n",
            "pytest                           7.4.4\n",
            "python-apt                       2.4.0\n",
            "python-box                       7.2.0\n",
            "python-dateutil                  2.8.2\n",
            "python-louvain                   0.16\n",
            "python-slugify                   8.0.4\n",
            "python-utils                     3.8.2\n",
            "pytz                             2024.2\n",
            "pyviz_comms                      3.0.3\n",
            "PyYAML                           6.0.2\n",
            "pyzmq                            24.0.1\n",
            "qdldl                            0.1.7.post4\n",
            "ratelim                          0.1.6\n",
            "referencing                      0.35.1\n",
            "regex                            2024.9.11\n",
            "requests                         2.32.3\n",
            "requests-oauthlib                1.3.1\n",
            "requirements-parser              0.9.0\n",
            "rich                             13.8.1\n",
            "rmm-cu12                         24.4.0\n",
            "rpds-py                          0.20.0\n",
            "rpy2                             3.4.2\n",
            "rsa                              4.9\n",
            "safetensors                      0.4.5\n",
            "scikit-image                     0.24.0\n",
            "scikit-learn                     1.5.2\n",
            "scipy                            1.13.1\n",
            "scooby                           0.10.0\n",
            "scs                              3.2.7\n",
            "seaborn                          0.13.1\n",
            "SecretStorage                    3.3.1\n",
            "Send2Trash                       1.8.3\n",
            "sentencepiece                    0.2.0\n",
            "setuptools                       71.0.4\n",
            "shapely                          2.0.6\n",
            "shellingham                      1.5.4\n",
            "simple-parsing                   0.1.6\n",
            "six                              1.16.0\n",
            "sklearn-pandas                   2.2.0\n",
            "smart-open                       7.0.4\n",
            "sniffio                          1.3.1\n",
            "snowballstemmer                  2.2.0\n",
            "sortedcontainers                 2.4.0\n",
            "soundfile                        0.12.1\n",
            "soupsieve                        2.6\n",
            "soxr                             0.5.0.post1\n",
            "spacy                            3.7.6\n",
            "spacy-legacy                     3.0.12\n",
            "spacy-loggers                    1.0.5\n",
            "Sphinx                           5.0.2\n",
            "sphinxcontrib-applehelp          2.0.0\n",
            "sphinxcontrib-devhelp            2.0.0\n",
            "sphinxcontrib-htmlhelp           2.1.0\n",
            "sphinxcontrib-jsmath             1.0.1\n",
            "sphinxcontrib-qthelp             2.0.0\n",
            "sphinxcontrib-serializinghtml    2.0.0\n",
            "SQLAlchemy                       2.0.35\n",
            "sqlglot                          25.1.0\n",
            "sqlparse                         0.5.1\n",
            "srsly                            2.4.8\n",
            "stanio                           0.5.1\n",
            "statsmodels                      0.14.3\n",
            "StrEnum                          0.4.15\n",
            "sympy                            1.13.3\n",
            "tables                           3.8.0\n",
            "tabulate                         0.9.0\n",
            "tbb                              2021.13.1\n",
            "tblib                            3.0.0\n",
            "tenacity                         9.0.0\n",
            "tensorboard                      2.17.0\n",
            "tensorboard-data-server          0.7.2\n",
            "tensorflow                       2.17.0\n",
            "tensorflow-datasets              4.9.6\n",
            "tensorflow-hub                   0.16.1\n",
            "tensorflow-io-gcs-filesystem     0.37.1\n",
            "tensorflow-metadata              1.15.0\n",
            "tensorflow-probability           0.24.0\n",
            "tensorstore                      0.1.65\n",
            "termcolor                        2.4.0\n",
            "terminado                        0.18.1\n",
            "text-unidecode                   1.3\n",
            "textblob                         0.17.1\n",
            "tf_keras                         2.17.0\n",
            "tf-slim                          1.1.0\n",
            "thinc                            8.2.5\n",
            "threadpoolctl                    3.5.0\n",
            "tifffile                         2024.9.20\n",
            "tinycss2                         1.3.0\n",
            "tokenizers                       0.19.1\n",
            "toml                             0.10.2\n",
            "tomli                            2.0.1\n",
            "toolz                            0.12.1\n",
            "torch                            2.4.1+cu121\n",
            "torchaudio                       2.4.1+cu121\n",
            "torchsummary                     1.5.1\n",
            "torchvision                      0.19.1+cu121\n",
            "tornado                          6.3.3\n",
            "tqdm                             4.66.5\n",
            "traitlets                        5.7.1\n",
            "traittypes                       0.2.1\n",
            "transformers                     4.44.2\n",
            "tweepy                           4.14.0\n",
            "typeguard                        4.3.0\n",
            "typer                            0.12.5\n",
            "types-pytz                       2024.2.0.20240913\n",
            "types-setuptools                 75.1.0.20240917\n",
            "typing_extensions                4.12.2\n",
            "tzdata                           2024.1\n",
            "tzlocal                          5.2\n",
            "uc-micro-py                      1.0.3\n",
            "uritemplate                      4.1.1\n",
            "urllib3                          2.2.3\n",
            "vega-datasets                    0.9.0\n",
            "wadllib                          1.3.6\n",
            "wasabi                           1.1.3\n",
            "wcwidth                          0.2.13\n",
            "weasel                           0.4.1\n",
            "webcolors                        24.8.0\n",
            "webencodings                     0.5.1\n",
            "websocket-client                 1.8.0\n",
            "Werkzeug                         3.0.4\n",
            "wheel                            0.44.0\n",
            "widgetsnbextension               3.6.9\n",
            "wordcloud                        1.9.3\n",
            "wrapt                            1.16.0\n",
            "xarray                           2024.9.0\n",
            "xarray-einstats                  0.8.0\n",
            "xgboost                          2.1.1\n",
            "xlrd                             2.0.1\n",
            "xyzservices                      2024.9.0\n",
            "yarl                             1.11.1\n",
            "yellowbrick                      1.5\n",
            "yfinance                         0.2.43\n",
            "zict                             3.0.0\n",
            "zipp                             3.20.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE9Pd103SErM",
        "outputId": "03fd9ce5-70c8-433c-ca0e-cfb1fe219d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpvPCNz9UIDx"
      },
      "source": [
        "This code imports necessary libraries and modules for building and training Convolutional Neural Networks (ConvNets) using Keras and TensorFlow. It includes layers for constructing ConvNets, such as convolutional, pooling, activation, and normalization layers. The code also imports utilities for data preprocessing, including loading the CIFAR-100 dataset and preprocessing images. Additionally, it imports ResNet-50, a pre-trained ConvNet model. Overall, this code sets up the environment and tools needed for building and training ConvNets on image classification tasks, leveraging the CIFAR-100 dataset and ResNet-50 architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oiVZ8VX8RY0p"
      },
      "outputs": [],
      "source": [
        "# Import necessary layers and utilities from Keras\n",
        "from keras import layers  # General layers import for neural networks\n",
        "from keras.layers import (Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization,\n",
        "                          Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,\n",
        "                          GlobalAveragePooling2D, Dropout, UpSampling2D)\n",
        "# These are specific layers and operations used to build neural networks\n",
        "# - Input: defines input tensor for a model\n",
        "# - Add: element-wise addition of tensors\n",
        "# - Dense: fully connected layer\n",
        "# - Activation: activation functions (like ReLU, sigmoid, etc.)\n",
        "# - ZeroPadding2D: padding layer to add zero-padding around the inputs\n",
        "# - BatchNormalization: normalizes activations and gradients to help with training stability\n",
        "# - Conv2D: 2D convolution layer for processing image data\n",
        "# - Pooling layers (AveragePooling2D, MaxPooling2D): reduce spatial dimensions of feature maps\n",
        "# - GlobalMaxPooling2D, GlobalAveragePooling2D: reduces each feature map to a single value (global pooling)\n",
        "# - Dropout: regularization layer to prevent overfitting\n",
        "# - UpSampling2D: increases spatial dimensions by upsampling\n",
        "\n",
        "# Import Model-related classes from Keras\n",
        "from keras.models import Model, load_model, Sequential\n",
        "# - Model: general class for creating neural networks\n",
        "# - load_model: loads a previously saved model from disk\n",
        "# - Sequential: sequential model where layers are stacked one after the other\n",
        "\n",
        "# Import initializer\n",
        "from keras.initializers import glorot_uniform  # Xavier/Glorot uniform initializer for setting initial weights\n",
        "\n",
        "# Import the CIFAR-100 dataset\n",
        "from keras.datasets import cifar100  # CIFAR-100 dataset, which contains 100 classes of images\n",
        "\n",
        "# Import preprocessing function for ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input  # Preprocess input images for ResNet50\n",
        "\n",
        "# Import utilities for one-hot encoding and image processing\n",
        "from tensorflow.keras.utils import to_categorical  # Converts labels to one-hot encoding\n",
        "\n",
        "# Import additional libraries\n",
        "import numpy as np  # Numpy for numerical operations\n",
        "import tensorflow as tf  # TensorFlow as backend for Keras models\n",
        "\n",
        "# Import class for augmenting image data\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For data augmentation (randomly transforming images during training)\n",
        "\n",
        "# Import ResNet50 model from Keras\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50  # Pre-trained ResNet50 architecture for transfer learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzk1TLpxbyrv"
      },
      "source": [
        "# Challenge of Vanishing Gradients in Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhiCgBfBS4Cm"
      },
      "source": [
        "Deep neural networks have the capacity to learn features across various levels of abstraction, ranging from simple features like edges, which are captured in the shallower layers near the input, to highly intricate features found in the deeper layers closer to the output. However, increasing the depth of a network doesn't consistently improve its performance. A significant challenge in training deeper networks is the occurrence of vanishing gradients, where the gradient signal diminishes rapidly during backpropagation, rendering gradient descent impractically slow. Specifically, as you propagate gradients from the final layer back to the initial layer, the multiplication by weight matrices at each step can cause the gradient to diminish exponentially, leading it to approach zero. Consequently, during training, you may observe the gradient magnitude or norm for shallower layers declining rapidly towards zero as the training progresses.\n",
        "\n",
        "Residual Networks (ResNets) employ two primary types of blocks: \"identity blocks\" and \"convolutional blocks\". Identitiy blocks are known as \"shortcuts\" or \"skip connections\", which enable the model to bypass certain layers, as depicted in Figures 1 and 2.\n",
        "\n",
        "![picture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ME62WUBfdIcLIWoCHJekhg.jpeg)\n",
        "\n",
        "*Figure 1: ResNets Vs Plain Neural Network taken from [this article](https://towardsdatascience.com/residual-networks-resnets-cb474c7c834a).*\n",
        "\n",
        "![picture](https://theaisummer.com/static/8d19d048cd68d6dce362e025cf3b635a/1ac66/skip-connection.png)\n",
        "\n",
        "*Figure 2: Residual learning: a building block taken from [original ResNet paper](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf).*\n",
        "\n",
        "ResNets work by allowing the model to easily learn from previous layers. They do this by adding the original input of a layer to its output, essentially creating a shortcut. This way, the gradient during training can flow smoothly back through the network, helping earlier layers learn better and preventing the vanishing gradients problem. By stacking these shortcut blocks, ResNets can efficiently learn from different parts of the input data. In addition to addressing the issue of vanishing gradients, ResNets are commonly used because they allow later layers to learn from information captured in the initial layers.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14_R6R6bmlN"
      },
      "source": [
        "# Building a Residual Network (ResNet) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueWCPzuddZB2"
      },
      "source": [
        "In ResNets, there are two main types of blocks: the identity block and the convolutional block.\n",
        "\n",
        " - Identity Block: This block is used when the input and output dimensions remain the same. It consists of a series of convolutional layers and other operations, but the shortcut connection (which bypasses these layers) simply adds the input directly to the output. This preserves the dimensionality of the input throughout the block.\n",
        "\n",
        "- Convolutional Block: This block is used when the input and output dimensions do not match up, meaning the convolutional layers inside the block change the dimensions of the input. In contrast to the identity block, the convolutional block incorporates a convolutional layer in the shortcut path. This convolutional layer adjusts the dimensions of the input to match the dimensions of the output so that they can be added together. This ensures that the dimensions align properly for addition and facilitates the flow of information through the network.\n",
        "\n",
        "  ![picture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhQQdqZ_XciBou1yAPL8ow.jpeg)\n",
        "  ![picture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NX7Bhwa5yVbA27LRKFLiIQ.jpeg)\n",
        "\n",
        "  *Figure 3: ResNet50 Identity Block (top figure) and Conv Block (bottom figure) taken from this [article](https://towardsdatascience.com/residual-networks-resnets-cb474c7c834a).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9cB6stycMgO"
      },
      "source": [
        "## The Identity Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEJuOfb6Zp8e"
      },
      "source": [
        "Here is how the identity_block function is implemented:\n",
        "\n",
        "- It sets up naming conventions for the layers.\n",
        "- Extracts the sizes of filters for each convolution layer.\n",
        "- Saves the input tensor for later addition (shortcut connection).\n",
        "- Builds the main path of the block:\n",
        "  - First, it applies a 1x1 convolution with F1 filters, followed by batch normalization and ReLU activation.\n",
        "  - Then, it applies a convolution with a filter size of filter_size and F2 filters, followed by batch normalization and ReLU activation.\n",
        "  - Finally, it applies another 1x1 convolution with F3 filters, followed by batch normalization.\n",
        "- Adds the shortcut connection to the main path. In specific, after the third convolutional layer, the output is added to the preserved input tensor X_shortcut. This addition operation effectively creates a skip connection that bypasses (or \"skips over\") the three convolutional layers in the main path, allowing the input tensor to directly influence the final output of the block.\n",
        "- Applies ReLU activation to the final output.\n",
        "\n",
        "The function returns the output tensor of the identity block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xuueMp4NZqqJ"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: identity_block\n",
        "\n",
        "def identity_block(X, filter_size, num_filters, stage, block):\n",
        "    \"\"\"\n",
        "    Implements an identity block for a residual network (ResNet).\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor with shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    filter_size -- integer indicating the size of the middle convolution window in the main path\n",
        "    num_filters -- list of integers specifying the number of filters for each convolution layer in the block\n",
        "    stage -- integer used for naming layers according to the stage in the network\n",
        "    block -- string/character used for naming layers within the stage\n",
        "\n",
        "    Returns:\n",
        "    X -- output tensor after passing through the identity block\n",
        "    \"\"\"\n",
        "\n",
        "    # Naming convention to keep track of layers: res{stage}{block}_branch{layer}\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Unpack the number of filters for each convolution layer in the block\n",
        "    F1, F2, F3 = num_filters\n",
        "\n",
        "    # Save the input value for later (shortcut path)\n",
        "    X_shortcut = X\n",
        "\n",
        "    # FIRST COMPONENT OF MAIN PATH\n",
        "    # 1x1 Convolution to reduce dimensions (bottleneck layer), followed by BatchNormalization and ReLU activation\n",
        "    X = Conv2D(F1, 1, padding='valid', name=conv_name_base + '2a')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)  # Normalize the activations\n",
        "    X = Activation('relu')(X)  # Apply ReLU activation\n",
        "\n",
        "    # SECOND COMPONENT OF MAIN PATH\n",
        "    # 3x3 Convolution (with padding to preserve spatial dimensions), followed by BatchNormalization and ReLU activation\n",
        "    X = Conv2D(F2, filter_size, padding='same', name=conv_name_base + '2b')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)  # Normalize activations\n",
        "    X = Activation('relu')(X)  # Apply ReLU activation\n",
        "\n",
        "    # THIRD COMPONENT OF MAIN PATH\n",
        "    # 1x1 Convolution to restore original dimensions (bottleneck layer), followed by BatchNormalization\n",
        "    X = Conv2D(F3, 1, padding='valid', name=conv_name_base + '2c')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)  # Normalize the activations\n",
        "\n",
        "    # FINAL STEP: Add the shortcut value (X_shortcut) to the main path (X)\n",
        "    X = Add()([X, X_shortcut])  # Skip connection: adds the input to the output of the 3-layer path\n",
        "    X = Activation('relu')(X)  # Apply ReLU activation to the combined output\n",
        "\n",
        "    return X  # Return the final output of the identity block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS2RtRg1cmBM"
      },
      "source": [
        "## The Convolutional Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5cpAtJ3lZBS"
      },
      "source": [
        "Here is how the convolutional block in ResNet is implemented:\n",
        "\n",
        "- Function Definition: The function convolutional_block takes several arguments including the input tensor X, the filter size, the number of filters, stage number, block identifier, and an optional stride value. It returns the output tensor of the convolutional block.\n",
        "\n",
        "- Naming Conventions: The function sets up naming conventions for the layers based on the stage and block numbers.\n",
        "\n",
        "- Extracting Filter Sizes: The sizes of the filters for each convolutional layer (F1, F2, F3) are extracted from the num_filters argument.\n",
        "\n",
        "- Preserving the Input: The input tensor X is saved as X_shortcut to be used later for the shortcut connection.\n",
        "\n",
        "- Main Path Construction:\n",
        "  - First, a 1x1 convolution is applied with F1 filters and a specified stride (default is 2), followed by batch normalization and ReLU activation.\n",
        "  - Then, a convolution with a filter size specified by filter_size and F2 filters is applied, followed by batch normalization and ReLU activation.\n",
        "  - Finally, another 1x1 convolution is applied with F3 filters, followed by batch normalization.\n",
        "\n",
        "- Shortcut Path Construction: A 1x1 convolution is applied to the preserved input tensor X_shortcut with F3 filters and the same stride as the main path, followed by batch normalization.\n",
        "\n",
        "- Final Step: The output of the main path and the shortcut path are added element-wise. ReLU activation is applied to the summed output.\n",
        "\n",
        "- Return: The final output tensor of the convolutional block is returned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0VemCrbalXY3"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: convolutional_block\n",
        "\n",
        "def convolutional_block(X, filter_size, num_filters, stage, block, stride=2):\n",
        "    \"\"\"\n",
        "    Implements a convolutional block for a residual network (ResNet).\n",
        "    Unlike the identity block, this block changes the dimensions of the input.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    filter_size -- size of the 3x3 convolution window in the main path\n",
        "    num_filters -- list of integers specifying the number of filters for each convolution layer in the main path\n",
        "    stage -- integer used to name layers based on their position in the network\n",
        "    block -- string/character used to name layers within the stage\n",
        "    stride -- integer specifying the stride applied to the convolution\n",
        "\n",
        "    Returns:\n",
        "    X -- output tensor of the convolutional block with shape (n_H, n_W, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    # Naming convention to keep track of layers: res{stage}{block}_branch{layer}\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Unpack the number of filters for each convolution layer in the block\n",
        "    F1, F2, F3 = num_filters\n",
        "\n",
        "    # Save the input tensor (X) for later use in the shortcut path\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First layer: 1x1 Convolution with a stride (used to downsample), followed by BatchNormalization and ReLU activation\n",
        "    X = Conv2D(F1, 1, strides=(stride, stride), name=conv_name_base + '2a', kernel_initializer='glorot_uniform')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)  # Normalize the activations\n",
        "    X = Activation('relu')(X)  # Apply ReLU activation\n",
        "\n",
        "    # Second layer: 3x3 Convolution (preserving spatial dimensions with padding='same'), followed by BatchNormalization and ReLU activation\n",
        "    X = Conv2D(F2, filter_size, padding='same', name=conv_name_base + '2b', kernel_initializer='glorot_uniform')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)  # Normalize the activations\n",
        "    X = Activation('relu')(X)  # Apply ReLU activation\n",
        "\n",
        "    # Third layer: 1x1 Convolution to restore the depth to F3, followed by BatchNormalization\n",
        "    X = Conv2D(F3, 1, name=conv_name_base + '2c', kernel_initializer='glorot_uniform')(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)  # Normalize the activations\n",
        "\n",
        "    ##### SHORTCUT PATH ####\n",
        "    # Shortcut path: Adjusts the input dimensions using a 1x1 Convolution with a stride to match the output shape of the main path\n",
        "    X_shortcut = Conv2D(F3, 1, strides=(stride, stride), name=conv_name_base + '1', kernel_initializer='glorot_uniform')(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)  # Normalize the shortcut\n",
        "\n",
        "    ##### FINAL STEP #####\n",
        "    # Add the shortcut path output to the main path's final output, followed by ReLU activation\n",
        "    X = Add()([X, X_shortcut])  # Skip connection: Adds the input (shortcut) to the processed output\n",
        "    X = Activation('relu')(X)  # Apply ReLU activation to the combined output\n",
        "\n",
        "    return X  # Return the final output of the convolutional block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVRdaoOfnY8V"
      },
      "source": [
        "## Building ResNet model with 50 layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o00keDuYnezz"
      },
      "source": [
        "The ResNet_50 function builds the ResNet-50 architecture by stacking convolutional blocks and identity blocks, incorporating skip connections to facilitate training of very deep networks, and ends with a global average pooling layer and a fully connected output layer for classification.\n",
        "\n",
        "- Function Definition: ResNet_50 function takes two arguments: input_shape, which specifies the shape of the input images, and classes, the number of output classes. It returns a Keras Model instance representing the ResNet-50 model.\n",
        "\n",
        "- Input Tensor: It starts by defining an input tensor X_input with the shape specified by input_shape.\n",
        "\n",
        "- Zero Padding: Zero-padding is applied to the input tensor to make sure the spatial dimensions remain consistent after the initial convolution. Without padding, the image would get smaller as you go deeper into the network. Padding helps prevent information loss at the edges of the image. In addition, it makes sure that every pixel in the image is treated equally by the filters, which helps the network learn better and more meaningful patterns.\n",
        "\n",
        "- Stage 1: The initial convolutional layer (conv1) applies 64 filters of size 7x7 to the input, followed by batch normalization and ReLU activation.\n",
        "Max-pooling is then applied to reduce the spatial dimensions.\n",
        "\n",
        "- Stage 2: It constructs Stage 2, which consists of a convolutional block followed by two identity blocks. The convolutional block applies a convolutional layer followed by batch normalization and ReLU activation, then a shortcut connection is added through the identity blocks.\n",
        "\n",
        "- Stage 3, Stage 4, Stage 5: Similar to Stage 2, but with different numbers of convolutional and identity blocks, each with varying numbers of filters and strides.\n",
        "\n",
        "- Average Pooling: Average pooling with a pool size of 1x1 is applied to reduce the spatial dimensions to 1x1, effectively summarizing the features.\n",
        "\n",
        "- Output Layer: The output of the pooling layer is flattened and fed into a fully connected layer with softmax activation, producing the final output probabilities for each class.\n",
        "\n",
        "- Finally, a Keras Model instance is created with the input and output tensors defined earlier, and the model is named 'ResNet-50'.\n",
        "\n",
        "Here's how the layers are distributed in ResNet-50:\n",
        "\n",
        "- Initial Convolutional Layer (Stage 1): 1 layer\n",
        "- Stage 2:\n",
        "  - 1 Convolutional Block (3 layers)\n",
        "  - 2 Identity Blocks (2 * 3 = 6 layers). *Note that each identity block consists of 3 layers (1 convolutional layer + 1 batch normalization layer + 1 ReLU activation layer).*\n",
        "- Stage 3:\n",
        "  - 1 Convolutional Block (3 layers)\n",
        "  - 3 Identity Blocks (3 * 3 = 9 layers)\n",
        "- Stage 4:\n",
        "  - 1 Convolutional Block (3 layers)\n",
        "  - 5 Identity Blocks (5 * 3 = 15 layers)\n",
        "- Stage 5:\n",
        "  - 1 Convolutional Block (3 layers)\n",
        "  - 2 Identity Blocks (2 * 3 = 6 layers)\n",
        "\n",
        " Adding up all these layers gives us a total of 49 layers.\n",
        "\n",
        "To achieve a total of 50 layers, ResNet-50 also includes fully connected layers (classification layers) at the end, which contribute an additional 1 layer.\n",
        "\n",
        "Below is the architecture of ResNet-50.\n",
        "\n",
        "![picture](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tH9evuOFqk8F41FG.png)\n",
        "\n",
        "*Figure 4: Resnet-50 Model architecture taken from this [article](https://towardsdatascience.com/the-annotated-resnet-50-a6c536034758)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JkCxs2-FoFgj"
      },
      "outputs": [],
      "source": [
        "def ResNet_50(input_shape=(32, 32, 3), classes=100):\n",
        "    \"\"\"\n",
        "    Implementation of ResNet50 architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images in the dataset (height, width, channels)\n",
        "    classes -- integer, number of output classes for classification\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras representing the ResNet50 architecture\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the input as a tensor with the specified input shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # Zero padding: Padding 3 pixels on the top, bottom, left, and right of the input\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # Stage 1\n",
        "    # CONV2D: 7x7 convolution filter, stride of 2 for downsampling, followed by batch normalization and ReLU activation\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # MAXPOOL: 3x3 max pooling with a stride of 2 for further downsampling\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    # Convolutional block: Changes dimensions with stride of 1, followed by two identity blocks\n",
        "    X = convolutional_block(X, filter_size=3, num_filters=[64, 64, 256], stage=2, block='a', stride=1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    # Convolutional block with a stride of 2 for downsampling, followed by three identity blocks\n",
        "    X = convolutional_block(X, 3, [128, 128, 512], stage=3, block='a', stride=2)\n",
        "    for i in range(3):\n",
        "        X = identity_block(X, 3, [128, 128, 512], stage=3, block=chr(98 + i))  # blocks b, c, d\n",
        "\n",
        "    # Stage 4\n",
        "    # Convolutional block with a stride of 2 for downsampling, followed by five identity blocks\n",
        "    X = convolutional_block(X, 3, [256, 256, 1024], stage=4, block='a', stride=2)\n",
        "    for i in range(5):\n",
        "        X = identity_block(X, 3, [256, 256, 1024], stage=4, block=chr(98 + i))  # blocks b, c, d, e, f\n",
        "\n",
        "    # Stage 5\n",
        "    # Convolutional block with a stride of 2 for downsampling, followed by two identity blocks\n",
        "    X = convolutional_block(X, 3, [512, 512, 2048], stage=5, block='a', stride=2)\n",
        "    for i in range(2):\n",
        "        X = identity_block(X, 3, [512, 512, 2048], stage=5, block=chr(98 + i))  # blocks b, c\n",
        "\n",
        "    # Average Pooling: Global average pooling to reduce the tensor dimensions\n",
        "    X = AveragePooling2D((1, 1), name=\"avg_pool\")(X)\n",
        "\n",
        "    # Fully connected layer (dense layer) with softmax activation for output classes\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "\n",
        "    # Create the Keras model\n",
        "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2aRTMvDsMYz"
      },
      "source": [
        "Run the following code to build the model's graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CbU7e3bEsMt6"
      },
      "outputs": [],
      "source": [
        "# Creating ResNet50 model\n",
        "model = ResNet_50(input_shape = (32, 32, 3), classes = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uetvZ5WLsl3l"
      },
      "source": [
        "Before training a model, you must set up the learning process by compiling the model.\n",
        "\n",
        "The following line of code performs the following tasks:\n",
        "\n",
        "**Optimizer (adam):**\n",
        "You are using the Adam optimizer, which is an adaptive learning rate optimizer. It adjusts the learning rate for each parameter based on estimates of lower-order moments (mean and variance) of gradients, providing better performance for many tasks.\n",
        "\n",
        "**Loss function (categorical_crossentropy):**\n",
        "This loss function is used for multi-class classification problems where the target labels are in a one-hot encoded format. Each class is represented as a vector with a 1 for the correct class and 0 for the rest.\n",
        "\n",
        "**Metrics (accuracy):**\n",
        "The model is evaluated based on its accuracy during training and validation. This metric calculates the fraction of correctly classified samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LWJsVrVMsm68"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4yVdIuwtqqS"
      },
      "source": [
        "The model is prepared for training, and all that's required is a dataset. Now, let's load the CIFAR-100 Dataset. CIFAR-100 is a popular benchmark dataset for image classification tasks, consisting of 60,000 32x32 color images across 100 diverse classes, each containing 500 training and 100 testing images. It serves as a challenging testbed for evaluating machine learning algorithms due to its varied classes, including animals, vehicles, and household items, making it suitable for assessing model generalization and robustness in real-world scenarios.\n",
        "\n",
        "![picture](https://production-media.paperswithcode.com/datasets/CIFAR-100-0000000433-b71f61c0_hPEzMRg.jpg)\n",
        "\n",
        "*Figure 5: A snapshot of the CIFAR100 classes with 10 random images in each  taken from this [source](https://www.cs.toronto.edu/~kriz/cifar.html)*\n",
        "\n",
        "The following code loads the CIFAR-100 dataset. It preprocesses the input images using the preprocess_input function to ensure compatibility with the ResNet-50 model. Additionally, it converts the class labels to one-hot encoded vectors using the to_categorical function, which is essential for categorical classification tasks. Finally, it prints out the number of training and test examples, as well as the shapes of the input and output data arrays, to verify the data preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8wNXbJCtrxw",
        "outputId": "997d04a2-dbf0-4dbd-9f30-3238e535ee1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "number of training examples = 50000\n",
            "number of test examples = 10000\n",
            "X_train shape: (50000, 32, 32, 3)\n",
            "Y_train shape: (50000, 100)\n",
            "X_test shape: (10000, 32, 32, 3)\n",
            "Y_test shape: (10000, 100)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 100\n",
        "\n",
        "# Load the CIFAR-100 dataset, consisting of 100 classes for training and testing\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar100.load_data()\n",
        "\n",
        "# Pre-process the data\n",
        "# Apply preprocessing specific to ResNet50 (normalizes pixel values to the range used by the model)\n",
        "X_train = preprocess_input(X_train)\n",
        "X_test = preprocess_input(X_test)\n",
        "\n",
        "# Convert class labels (Y_train and Y_test) to one-hot encoded format\n",
        "# This is required for the categorical_crossentropy loss function, as it expects labels in one-hot format\n",
        "Y_train = to_categorical(Y_train, num_classes)\n",
        "Y_test = to_categorical(Y_test, num_classes)\n",
        "\n",
        "# Print information about the dataset shapes\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))  # Displays the number of training examples\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))        # Displays the number of test examples\n",
        "print (\"X_train shape: \" + str(X_train.shape))                    # Shows the shape of the training data\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))                    # Shows the shape of the one-hot encoded training labels\n",
        "print (\"X_test shape: \" + str(X_test.shape))                      # Shows the shape of the test data\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))                      # Shows the shape of the one-hot encoded test labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5iQPC7vxXrY"
      },
      "source": [
        "Execute the code below to train your model for 2 epochs using a batch size of 32.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8hMbkCzxizi",
        "outputId": "9f5ca797-28fe-412d-e064-34f140a3b6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 43ms/step - accuracy: 0.0424 - loss: 5.4987\n",
            "Epoch 2/2\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 31ms/step - accuracy: 0.1015 - loss: 4.3302\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d7edfbec9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Train the model on the training data\n",
        "# Arguments:\n",
        "# - X_train: training data (images)\n",
        "# - Y_train: training labels (one-hot encoded)\n",
        "# - epochs: number of iterations over the entire training data (in this case, 2)\n",
        "# - batch_size: number of samples processed before the model updates its weights (32 images at a time)\n",
        "model.fit(X_train, Y_train, epochs=2, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxufked3aTEf"
      },
      "source": [
        "Let's observe the performance of this model, which has been trained for just two epochs, on the test dataset. We can observe that the test accuracy is very low since we only train it for 2 epochs. In the next section, we will leverage transfer learning to enhance the model's performance. By leveraging a pre-trained model, such as ResNet-50 trained on ImageNet, we can initialize our model with learned features and fine-tune it on the CIFAR-100 dataset. This approach often leads to significant improvements in model accuracy and generalization, even with limited training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgp2rqBnaU1T",
        "outputId": "dc1f6e6e-774d-4772-a7cf-3b93d7ba532d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.0271 - loss: 6.8104\n",
            "Loss = 6.941950798034668\n",
            "Test Accuracy = 0.027400000020861626\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test data\n",
        "# Arguments:\n",
        "# - X_test: test data (images) to evaluate the model's performance\n",
        "# - Y_test: true labels for the test data\n",
        "# The evaluate method returns the loss value and metrics specified during compilation\n",
        "preds = model.evaluate(X_test, Y_test)\n",
        "\n",
        "# Print the loss and test accuracy\n",
        "print(\"Loss = \" + str(preds[0]))  # preds[0] contains the loss value\n",
        "print(\"Test Accuracy = \" + str(preds[1]))  # preds[1] contains the accuracy of the model on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMuV-_flR9aQ"
      },
      "source": [
        "# Transfer Learning\n",
        "We will leverage transfer learning with the pre-trained ResNet-50 model to classify images in the CIFAR-100 dataset. The following code fine-tunes the model by adding new layers for classification and freezing the weights of the base model, achieving improved performance on the target task.\n",
        "\n",
        "- Data Preparation: we load the CIFAR-100 dataset and preprocesses the input images using the preprocess_input function to ensure compatibility with the ResNet-50 model.\n",
        "\n",
        "- Data Augmentation: An ImageDataGenerator is defined to perform data augmentation, which includes various transformations like rotation, zooming, and flipping. It is fitted to the training data.\n",
        "\n",
        "- Data Preprocessing: We convert the class labels to one-hot encoded vectors using the to_categorical function. We then resize the input images from CIFAR-100 dataset, which have size 32x32x3, to match the input shape expected by the ResNet-50 model (224x224x3) using the UpSampling2D layer. By upsampling the images by a factor of 7 (since 32 * 7 = 224), the input size is effectively increased to meet the requirements of the ResNet model pretrained on Imagenet dataset.\n",
        "\n",
        "- Model Loading: the pre-trained ResNet-50 model is loaded using the Keras library with specific configurations. By setting include_top=False, the fully connected layers at the top of the network, which are typically tailored for ImageNet classification, are excluded. The weights='imagenet' parameter initializes the model with weights pre-trained on the ImageNet dataset, providing a solid foundation for feature extraction. Additionally, the input shape takes the shape of the resized input images, which is (224x224x3).\n",
        "\n",
        "- Set Trainable Layers: It iterates through all the layers of the ResNet-50 model. If the layer is a batch normalization layer, it sets it to be trainable. Otherwise, it freezes the weights of the other layers by setting trainable=False. This is important because batch normalization relies on statistics calculated during training. Freezing these layers would prevent them from updating their statistics. Therefore, we retrain the batch normalization layer while keeping other layers fixed to ensure reliable predictions.\n",
        "\n",
        "- Model Definition: Defines a Sequential model and adds layers:\n",
        " - Adds the pre-trained ResNet50 model.\n",
        " - Adds a GlobalAveragePooling2D layer to reduce spatial dimensions.\n",
        " - Adds a Dense layer with 256 units and ReLU activation.\n",
        " - Adds a Dropout layer with a dropout rate of 0.25 to prevent overfitting.\n",
        " - Adds a BatchNormalization layer to normalize the activations of the previous layer.\n",
        " - Adds a Dense output layer with softmax activation for multi-class classification.\n",
        "\n",
        " *When you have a binary classification task, such as distinguishing between cats and dogs, you only need two units (neurons) in the output layer instead of 100.*\n",
        "\n",
        "- Model Compilation: The model is compiled with with categorical cross-entropy loss, Adam optimizer, and accuracy metric.\n",
        "\n",
        "- Model Training: The model is trained using the fit_generator function with data augmentation, specifying the batch size, number of training steps per epoch, number of epochs, and validation data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8ZQbzgBJeFq",
        "outputId": "167c5305-d580-4628-8861-20d0eddf93ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 456ms/step - accuracy: 0.3571 - loss: 2.6798 - val_accuracy: 0.6773 - val_loss: 1.0875\n",
            "Epoch 2/15\n",
            "\u001b[1m  1/781\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:08\u001b[0m 396ms/step - accuracy: 0.5469 - loss: 1.4707"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.5469 - loss: 1.4707 - val_accuracy: 0.6789 - val_loss: 1.0851\n",
            "Epoch 3/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 427ms/step - accuracy: 0.6528 - loss: 1.1805 - val_accuracy: 0.7312 - val_loss: 0.8980\n",
            "Epoch 4/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.5625 - loss: 1.1165 - val_accuracy: 0.7313 - val_loss: 0.8990\n",
            "Epoch 5/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 406ms/step - accuracy: 0.7073 - loss: 0.9841 - val_accuracy: 0.7389 - val_loss: 0.8623\n",
            "Epoch 6/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.7812 - loss: 0.8520 - val_accuracy: 0.7396 - val_loss: 0.8630\n",
            "Epoch 7/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 405ms/step - accuracy: 0.7357 - loss: 0.8765 - val_accuracy: 0.7599 - val_loss: 0.7864\n",
            "Epoch 8/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.6719 - loss: 1.0256 - val_accuracy: 0.7599 - val_loss: 0.7864\n",
            "Epoch 9/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 426ms/step - accuracy: 0.7584 - loss: 0.8019 - val_accuracy: 0.7594 - val_loss: 0.8050\n",
            "Epoch 10/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.6875 - loss: 1.1067 - val_accuracy: 0.7611 - val_loss: 0.8013\n",
            "Epoch 11/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 426ms/step - accuracy: 0.7724 - loss: 0.7359 - val_accuracy: 0.7675 - val_loss: 0.7913\n",
            "Epoch 12/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.7969 - loss: 0.7031 - val_accuracy: 0.7679 - val_loss: 0.7921\n",
            "Epoch 13/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 406ms/step - accuracy: 0.7870 - loss: 0.6859 - val_accuracy: 0.7796 - val_loss: 0.7397\n",
            "Epoch 14/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 32ms/step - accuracy: 0.7500 - loss: 0.8107 - val_accuracy: 0.7793 - val_loss: 0.7392\n",
            "Epoch 15/15\n",
            "\u001b[1m781/781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 426ms/step - accuracy: 0.7983 - loss: 0.6444 - val_accuracy: 0.7788 - val_loss: 0.7618\n"
          ]
        }
      ],
      "source": [
        "# Number of classes in the CIFAR-100 dataset\n",
        "num_classes = 100\n",
        "\n",
        "# Load the CIFAR-100 dataset, which contains training and testing images and their labels\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# Pre-process the data using the ResNet50 preprocessing function\n",
        "x_train = preprocess_input(x_train)  # Normalize training images\n",
        "x_test = preprocess_input(x_test)    # Normalize test images\n",
        "\n",
        "# Define an ImageDataGenerator for data augmentation\n",
        "# This helps improve model generalization by applying transformations to the training images\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,          # Randomly rotate images in the range (degrees, 0 to 10)\n",
        "    zoom_range=0.1,            # Randomly zoom into images by a factor of 0.1\n",
        "    width_shift_range=0.1,     # Randomly shift images horizontally by 10% of the width\n",
        "    height_shift_range=0.1,    # Randomly shift images vertically by 10% of the height\n",
        "    shear_range=0.1,           # Shear transformations by 10%\n",
        "    horizontal_flip=True,       # Randomly flip images horizontally\n",
        "    vertical_flip=False         # Do not flip images vertically\n",
        ")\n",
        "\n",
        "# Fit the ImageDataGenerator to the training data\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Convert labels to categorical format (one-hot encoding)\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Input layer to resize input images to (224, 224) for ResNet50\n",
        "input_layer = Input(shape=(32, 32, 3))  # Original input shape\n",
        "resized_input = UpSampling2D(size=(7, 7))(input_layer)  # Upsample to (224, 224)\n",
        "\n",
        "# Load ResNet50 model with pre-trained weights and modified input shape\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_tensor=resized_input)\n",
        "\n",
        "# Freeze layers in the pre-trained ResNet50 model to retain learned features\n",
        "for layer in resnet_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):  # Allow BatchNormalization layers to be trainable\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False  # Freeze other layers\n",
        "\n",
        "# Define a Sequential model and add layers\n",
        "model = Sequential()\n",
        "model.add(resnet_model)                      # Add the ResNet50 model\n",
        "model.add(GlobalAveragePooling2D())         # Global average pooling to reduce dimensions\n",
        "model.add(Dense(256, activation='relu'))     # Fully connected layer with 256 units\n",
        "model.add(Dropout(0.25))                     # Dropout layer for regularization\n",
        "model.add(BatchNormalization())               # Batch normalization to stabilize learning\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Output layer for classification\n",
        "\n",
        "# Compile the model with loss function, optimizer, and metrics\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the updated fit method with data augmentation\n",
        "history = model.fit(\n",
        "    datagen.flow(x_train, y_train, batch_size=64),  # Data generator\n",
        "    steps_per_epoch=x_train.shape[0] // 64,  # Number of steps per epoch\n",
        "    epochs=15,  # Number of epochs\n",
        "    validation_data=(x_test, y_test)  # Validation data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation: The model is evaluated on the test data and the test loss and accuracy are printed. Note that the model can be further improved by increasing the number of epochs."
      ],
      "metadata": {
        "id": "R0TkRCcsr699"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1LqL0SmulekF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5872101-76a6-4772-ca57-dca5a7d64597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.7617626190185547\n",
            "Test accuracy: 0.7788000106811523\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG6nnQYVXate"
      },
      "source": [
        "The following code demonstrates how to use the pre-trained model to make predictions on an image of your choice. It loads the image, preprocesses it to match the model's input size and format, and then uses the trained model to predict the class probabilities for each of the 100 classes. Finally, it prints the top 5 predicted class labels of the test image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OVxbSuIWW5da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f15c3bf-7dc1-42bb-8408-04cfec5ba196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "Top 5 predicted classes:\n",
            "leopard\n",
            "tiger\n",
            "wolf\n",
            "hamster\n",
            "possum\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-100 class labels\n",
        "cifar100_labels = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard',\n",
        "    'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
        "    'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree',\n",
        "    'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "# Load an example image from the specified path\n",
        "img_path = '/content/yourimage.jpg'\n",
        "# Resize the image to match the input size of the model (32x32)\n",
        "img = image.load_img(img_path, target_size=(32, 32))\n",
        "# Convert the image to a numpy array\n",
        "img_array = image.img_to_array(img)\n",
        "# Add a batch dimension to the image array\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Preprocess the image by normalizing pixel values to the range expected by the model\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# Make predictions on the preprocessed image\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# Get the top 5 predicted class indices by sorting the predictions\n",
        "top_indices = np.argsort(predictions[0])[-5:][::-1]\n",
        "\n",
        "# Convert the top indices to corresponding class labels\n",
        "top_labels = [cifar100_labels[idx] for idx in top_indices]\n",
        "\n",
        "# Print the top 5 predicted class labels\n",
        "print(\"Top 5 predicted classes:\")\n",
        "for label in top_labels:\n",
        "    print(label)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}