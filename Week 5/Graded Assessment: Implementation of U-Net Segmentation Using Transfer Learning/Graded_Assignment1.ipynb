{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjjCdss52J0a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yek_WDX92ZOs"
      },
      "outputs": [],
      "source": [
        "# for data load\n",
        "import os\n",
        "\n",
        "# # Libraries for reading and processing images\n",
        "from IPython.display import Image, display\n",
        "from keras.utils import load_img\n",
        "from PIL import ImageOps\n",
        "from tensorflow import data as tf_data\n",
        "from tensorflow import image as tf_image\n",
        "from tensorflow import io as tf_io\n",
        "\n",
        "# for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np # for using np arrays\n",
        "import random\n",
        "\n",
        "# for bulding and running deep learning model\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-Net Decoder Block\n",
        "\n",
        "This code defines the decoder block for the U-Net architecture, which is responsible for upsampling the feature maps and merging them with skip connections from the corresponding encoder block. The block starts with a transpose convolution layer to increase the size of the feature maps. Then, it merges the upsampled feature maps with the skip connection from the encoder block to preserve spatial information. Finally, two convolutional layers with ReLU activation and HeNormal initialization are applied to refine the features. This decoder block plays a vital role in reconstructing high-resolution segmentation masks from the encoded features, enabling the U-Net to produce accurate segmentation results."
      ],
      "metadata": {
        "id": "3fGRykm-FDDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S8aseUECLdA"
      },
      "outputs": [],
      "source": [
        "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
        "    \"\"\"\n",
        "    Decoder Block first uses transpose convolution to upscale the image to a bigger size and then,\n",
        "    merges the result with skip layer results from encoder block\n",
        "    Adding 2 convolutions with 'same' padding helps further increase the depth of the network for better predictions\n",
        "    The function returns the decoded layer output\n",
        "    \"\"\"\n",
        "    # Start with a transpose convolution layer to first increase the size of the image\n",
        "    up = Conv2DTranspose(\n",
        "                 n_filters,\n",
        "                 (3,3),    # Kernel size\n",
        "                 strides=(2,2),\n",
        "                 padding='same')(prev_layer_input)\n",
        "\n",
        "    # Merge the skip connection from previous block to prevent information loss\n",
        "    merge = concatenate([up, skip_layer_input], axis=3)\n",
        "\n",
        "    # Add 2 Conv Layers with relu activation and HeNormal initialization for further processing\n",
        "    # The parameters for the function are similar to encoder\n",
        "    conv = Conv2D(n_filters,\n",
        "                 3,     # Kernel size\n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='HeNormal')(merge)\n",
        "    conv = Conv2D(n_filters,\n",
        "                 3,   # Kernel size\n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='HeNormal')(conv)\n",
        "    return conv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-Net Encoder Block\n",
        "\n",
        "This code defines the encoder block for the U-Net architecture, commonly used in image segmentation tasks. The block consists of two convolutional layers with ReLU activation and batch normalization for feature extraction. Optionally, dropout regularization is applied to prevent overfitting. If specified, max pooling is performed to downsample the spatial dimensions of the input. Additionally, a skip connection is established to preserve spatial information for the decoder part of the network. This encoder block plays a crucial role in extracting hierarchical features from the input image, facilitating the U-Net's ability to capture detailed information for accurate segmentation."
      ],
      "metadata": {
        "id": "oSFdec1HL9b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
        "    \"\"\"\n",
        "    This block uses multiple convolution layers, max pool, relu activation to create an architecture for learning.\n",
        "    Dropout can be added for regularization to prevent overfitting.\n",
        "    The block returns the activation values for next layer along with a skip connection which will be used in the decoder\n",
        "    \"\"\"\n",
        "    # Add 2 Conv Layers with relu activation and HeNormal initialization using TensorFlow\n",
        "    # Proper initialization prevents from the problem of exploding and vanishing gradients\n",
        "    # 'Same' padding will pad the input to conv layer such that the output has the same height and width (hence, is not reduced in size)\n",
        "    conv = Conv2D(n_filters,\n",
        "                  3,   # Kernel size\n",
        "                  activation='relu',\n",
        "                  padding='same',\n",
        "                  kernel_initializer='HeNormal')(inputs)\n",
        "    conv = Conv2D(n_filters,\n",
        "                  3,   # Kernel size\n",
        "                  activation='relu',\n",
        "                  padding='same',\n",
        "                  kernel_initializer='HeNormal')(conv)\n",
        "\n",
        "    # Batch Normalization will normalize the output of the last layer based on the batch's mean and standard deviation\n",
        "    conv = BatchNormalization()(conv, training=False)\n",
        "\n",
        "    # In case of overfitting, dropout will regularize the loss and gradient computation to shrink the influence of weights on output\n",
        "    if dropout_prob > 0:\n",
        "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
        "\n",
        "    # Pooling reduces the size of the image while keeping the number of channels same\n",
        "    # Pooling has been kept as optional as the last encoder layer does not use pooling (hence, makes the encoder block flexible to use)\n",
        "    # Below, Max pooling considers the maximum of the input slice for output computation and uses stride of 2 to traverse across input image\n",
        "    if max_pooling:\n",
        "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)\n",
        "    else:\n",
        "        next_layer = conv\n",
        "\n",
        "    # skip connection (without max pooling) will be input to the decoder layer to prevent information loss during transpose convolutions\n",
        "    skip_connection = conv\n",
        "\n",
        "    return next_layer, skip_connection"
      ],
      "metadata": {
        "id": "lSr91u59L4YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the U-Net Model (without transfer learning)\n",
        "\n",
        "This function constructs the complete U-Net model by combining both the encoder and decoder blocks. The encoder consists of multiple convolutional mini-blocks with increasing filter sizes and varying configurations for max-pooling and dropout. Each encoder block produces a skip connection that is passed to the corresponding decoder block. The decoder reverses the process, gradually decreasing the number of filters while upsampling the feature maps and incorporating skip connections to preserve spatial information. Finally, the model is completed with convolutional layers to produce the desired output size, with the number of channels matching the specified number of output classes. This compiled U-Net model is ready for training and inference on segmentation tasks."
      ],
      "metadata": {
        "id": "6QEPNHEAMPpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def UNetCompiled(input_size=(128, 128, 3), n_filters=32, n_classes=3):\n",
        "   \"\"\"\n",
        "   Combine both encoder and decoder blocks according to the U-Net research paper\n",
        "   Return the model as output\n",
        "   \"\"\"\n",
        "   # Input size represent the size of 1 image (the size used for pre-processing)\n",
        "   inputs = Input(input_size)\n",
        "\n",
        "   # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
        "   # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image\n",
        "   cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
        "   cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
        "   cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
        "   cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
        "   cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False)\n",
        "\n",
        "   # Decoder includes multiple mini blocks with decreasing number of filters\n",
        "   # Observe the skip connections from the encoder are given as input to the decoder\n",
        "   # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
        "   ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
        "   ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
        "   ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
        "   ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
        "\n",
        "   # Complete the model with 1 3x3 convolution layer (Same as the prev Conv Layers)\n",
        "   # Followed by a 1x1 Conv layer to get the image to the desired size.\n",
        "   # Observe the number of channels will be equal to number of output classes\n",
        "   conv9 = Conv2D(n_filters,\n",
        "                 3,\n",
        "                 activation='relu',\n",
        "                 padding='same',\n",
        "                 kernel_initializer='he_normal')(ublock9)\n",
        "\n",
        "   conv10 = Conv2D(n_classes, 1, padding='same')(conv9)\n",
        "\n",
        "   # Define the model\n",
        "   model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
        "\n",
        "   return model"
      ],
      "metadata": {
        "id": "NdAjPeRaMOIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the U-Net Model without Transfer Learning\n",
        "\n",
        "This code initializes the U-Net model by calling the UNetCompiled function, which defines the architecture of the neural network. The function takes parameters such as input size, number of filters, and number of output classes. In this case, the input image size is set to 128x128 with 3 channels (RGB), and the model is designed to have 32 filters in its layers and output into 3 classes. Additionally, the model is compiled, which involves configuring the training process with specific parameters such as loss function, optimizer, and metrics.\n",
        "\n",
        "The three classes typically correspond to:\n",
        "\n",
        "- Background: This class represents the background of the image, including areas that do not contain any pets.\n",
        "- Foreground/Pet: This class represents the main subject of interest in the image, which is typically a pet (such as a cat or a dog).\n",
        "- Boundary/Trimap: This class represents the boundary around the pet object, delineating its edges and helping to refine the segmentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Lj5VYFSMknQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the helper function for defining the layers for the model, given the input image size\n",
        "unet = UNetCompiled(input_size=(128,128,3), n_filters=32, n_classes=3)"
      ],
      "metadata": {
        "id": "HPZ53XTFMaDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graded Cell: U-Net model with a VGG16 encoder**\n",
        "\n",
        "Complete the following function named build_unet_with_vgg16_encoder that takes input_shape as an argument.\n",
        "\n",
        "- Input Layer:Define the input layer using the Input class with the provided input_shape.\n",
        "\n",
        "- Load VGG16 Model: Load the VGG16 model with pre-trained weights (imagenet). Make sure to exclude the top layers (include_top=False) and set the input_tensor to the previously defined input layer.\n",
        "\n",
        "- Extract Encoder Layers: Extract the relevant layers from the VGG16 model that will be used as the encoder part of the U-Net. These layers correspond to different blocks of convolutional layers in the VGG16 model.\n",
        "\n",
        "- Bridge Layer: Identify the output of the deepest convolutional layer in VGG16 (often from the fifth block) to serve as the bridge layer.\n",
        "\n",
        "- Build Decoder Layers: Use the DecoderMiniBlock function to build the decoder part of the U-Net, starting from the bridge layer and proceeding through the extracted encoder layers.\n",
        "\n",
        "- Final Convolutional Layers: Add a few more convolutional layers after the last decoder block to refine the final output. The last convolutional layer should reduce the number of channels to the number of desired output classes.\n",
        "\n",
        "- Compile the Model: Define the model using the Model class, specifying the input and output tensors. Return the compiled model.\n",
        "\n",
        "\n",
        "The following article might be helpful: Tomar, Nikhil. “VGG16 UNET Implementation in TensorFlow.” Idiot Developer, 3 Dec. 2021, [Article Link](idiotdeveloper.com/vgg16-unet-implementation-in-tensorflow/)."
      ],
      "metadata": {
        "id": "OKu3gRIPFOgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3-WvrB32zRz"
      },
      "outputs": [],
      "source": [
        "# Graded Cell\n",
        "def build_unet_with_vgg16_encoder(input_shape):\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the U-Net Model with Transfer Learning"
      ],
      "metadata": {
        "id": "_EooW0hoM3NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the U-Net model with VGG16 encoder\n",
        "model = build_unet_with_vgg16_encoder((128,128,3))"
      ],
      "metadata": {
        "id": "YX3jj3v5M1cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download and extract the Oxford-IIIT Pet Dataset. This dataset contains images of cats and dogs, along with annotations for pet breed and pixel-level segmentation masks. By downloading and extracting these files, we'll have access to the dataset for training and evaluating segmentation models."
      ],
      "metadata": {
        "id": "D2_QE9BZHiqd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0XjwQ1k28ln"
      },
      "outputs": [],
      "source": [
        "!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!\n",
        "!curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n",
        "!curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n",
        "!\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and Dataset Construction\n",
        "\n",
        "The following code loads images and their corresponding masks (annotations) from directories, preprocesses them, and constructs a TensorFlow dataset for training a segmentation model. It first collects file paths for images and masks, then defines functions to load and preprocess them using TensorFlow's IO and image processing modules. After defining a function to create a dataset from these paths, it constructs the dataset using specified batch size and image size. Finally, it iterates through the dataset, collecting batches of input images and target masks into arrays, which are then concatenated to form the complete input and target datasets for training the segmentation model."
      ],
      "metadata": {
        "id": "lNn6pSLrH7sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "\n",
        "input_img_paths = sorted([\n",
        "    os.path.join(input_dir, fname)\n",
        "    for fname in os.listdir(input_dir)\n",
        "    if fname.endswith(\".jpg\")\n",
        "])\n",
        "\n",
        "target_img_paths = sorted([\n",
        "    os.path.join(target_dir, fname)\n",
        "    for fname in os.listdir(target_dir)\n",
        "    if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "])\n",
        "\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)\n",
        "\n",
        "def load_img_masks(input_img_path, target_img_path):\n",
        "    input_img = tf_io.read_file(input_img_path)\n",
        "    input_img = tf_io.decode_jpeg(input_img, channels=3)\n",
        "    input_img = tf_image.resize(input_img, img_size)\n",
        "    input_img = tf_image.convert_image_dtype(input_img, tf.float32)\n",
        "\n",
        "    target_img = tf_io.read_file(target_img_path)\n",
        "    target_img = tf_io.decode_png(target_img, channels=1)\n",
        "    target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
        "    target_img = tf.squeeze(target_img, axis=-1)  # Remove the channel dimension\n",
        "    target_img = tf_image.convert_image_dtype(target_img, tf.uint8)\n",
        "\n",
        "    # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "    target_img -= 1\n",
        "    return input_img, target_img\n",
        "\n",
        "def get_dataset(\n",
        "    batch_size,\n",
        "    img_size,\n",
        "    input_img_paths,\n",
        "    target_img_paths,\n",
        "    max_dataset_len=None,\n",
        "):\n",
        "    \"\"\"Returns a TF Dataset.\"\"\"\n",
        "    # For faster debugging, limit the size of data\n",
        "    if max_dataset_len:\n",
        "        input_img_paths = input_img_paths[:max_dataset_len]\n",
        "        target_img_paths = target_img_paths[:max_dataset_len]\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
        "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "# Get the dataset\n",
        "dataset = get_dataset(\n",
        "    batch_size=batch_size,\n",
        "    img_size=img_size,\n",
        "    input_img_paths=input_img_paths,\n",
        "    target_img_paths=target_img_paths\n",
        ")\n",
        "\n",
        "# Collect batches from the dataset into arrays or tensors\n",
        "X_batches = []\n",
        "y_batches = []\n",
        "for X_batch, y_batch in dataset:\n",
        "    X_batches.append(X_batch.numpy())  # Convert TensorFlow tensor to numpy array\n",
        "    y_batches.append(y_batch.numpy())  # Convert TensorFlow tensor to numpy array\n",
        "\n",
        "# Concatenate batches into a single array or tensor\n",
        "X = np.concatenate(X_batches, axis=0)\n",
        "y = np.concatenate(y_batches, axis=0)\n"
      ],
      "metadata": {
        "id": "vVv7TtaQeAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use scikit-learn's function to split the dataset\n",
        "# 20% data as test/valid set\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "tYPGWEPKetNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the UNet Model for Image Segmentation on the Oxford Dataset\n",
        "\n",
        "This code compiles and trains a UNet model for image segmentation using the Oxford dataset with and without transfer learning. It configures the model with the Adam optimizer and sparse categorical crossentropy loss function, suitable for integer-based target labels. Additionally, it includes a ModelCheckpoint callback to save the best model during training and runs training for 20 epochs, validating the model's performance after each epoch."
      ],
      "metadata": {
        "id": "1kHWBzM-IdTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation_transfer_learning.keras\", save_best_only=True)\n",
        "]\n",
        "\n",
        "# Run the model in a mini-batch fashion and compute the progress for each epoch\n",
        "results_vgg16 = model.fit(X_train, y_train, batch_size=32, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid),callbacks=callbacks,)"
      ],
      "metadata": {
        "id": "IyaSVdnPilTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True)\n",
        "]\n",
        "\n",
        "# Run the model in a mini-batch fashion and compute the progress for each epoch\n",
        "results = unet.fit(X_train, y_train, batch_size=32, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid),callbacks=callbacks,)"
      ],
      "metadata": {
        "id": "OWgGVzQjNKh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying Predicted Masks Alongside Input Images and Actual Masks for the model without transfer learning"
      ],
      "metadata": {
        "id": "ZWWLDp84NbHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "val_preds = unet.predict(X_valid)\n",
        "\n",
        "def display_masks_side_by_side(input_image, actual_mask, predicted_mask):\n",
        "    \"\"\"Display input image, actual mask, and predicted mask side by side.\"\"\"\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Display input image\n",
        "    # Normalize input image to range [0, 1]\n",
        "    input_image_norm = input_image.astype(np.float32) / 255.0\n",
        "\n",
        "    axs[0].imshow(input_image_norm)\n",
        "    axs[0].set_title('Input Image')\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    # Display actual mask\n",
        "    axs[1].imshow(actual_mask)  # Remove indexing since actual_mask is a 2D array\n",
        "    axs[1].set_title('Actual Mask')\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    # Convert predicted mask to numpy array\n",
        "    predicted_mask_np = np.argmax(predicted_mask, axis=-1)\n",
        "\n",
        "    # Display predicted mask\n",
        "    axs[2].imshow(predicted_mask_np)\n",
        "    axs[2].set_title('Predicted Mask')\n",
        "    axs[2].axis('off')\n",
        "\n",
        "# Display results for validation image #10\n",
        "i = 10\n",
        "\n",
        "# Load input image (assuming X_valid contains the input images)\n",
        "input_image =X_valid[i]\n",
        "\n",
        "# Load actual mask (assuming y_valid contains the actual masks)\n",
        "actual_mask = y_valid[i]\n",
        "\n",
        "# Display input image, actual mask, and predicted mask side by side\n",
        "display_masks_side_by_side(input_image, actual_mask, val_preds[i])\n"
      ],
      "metadata": {
        "id": "VRIvXFZ5Ns6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Displaying Predicted Masks Alongside Input Images and Actual Masks for the model with transfer learning"
      ],
      "metadata": {
        "id": "k1q25MswNojz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "val_preds = model.predict(X_valid)\n",
        "\n",
        "# Display results for validation image #10\n",
        "i = 10\n",
        "\n",
        "# Load input image (assuming X_valid contains the input images)\n",
        "input_image =X_valid[i]\n",
        "\n",
        "# Load actual mask (assuming y_valid contains the actual masks)\n",
        "actual_mask = y_valid[i]\n",
        "\n",
        "# Display input image, actual mask, and predicted mask side by side\n",
        "display_masks_side_by_side(input_image, actual_mask, val_preds[i])\n"
      ],
      "metadata": {
        "id": "zGESfuXh67RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Evaluation Metrics\n",
        "\n",
        "Define the following evaluation metrics: precision, recall, accuracy, dice coefficient, and Intersection-over-Union (IoU).\n",
        "\n",
        "You can copy these definitions from this article: Huynh, Nghi. “Understanding Evaluation Metrics in Medical Image Segmentation.” Medium, 19 Mar. 2024. [Article Link](medium.com/@nghihuynh_37300/understanding-evaluation-metrics-in-medical-image-segmentation-d289a373a3f)."
      ],
      "metadata": {
        "id": "m-0AF6pTI4X5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFvVJE6LNa9J"
      },
      "outputs": [],
      "source": [
        "def precision_score_(groundtruth_mask, pred_mask):\n",
        "\n",
        "\n",
        "def recall_score_(groundtruth_mask, pred_mask):\n",
        "\n",
        "\n",
        "def accuracy(groundtruth_mask, pred_mask):\n",
        "\n",
        "\n",
        "def dice_coef(groundtruth_mask, pred_mask):\n",
        "\n",
        "\n",
        "def iou(groundtruth_mask, pred_mask):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Graded Cell: Model Evaluation**\n",
        "\n",
        "- Predict on the Validation Set: Use the trained model to make predictions on the validation dataset (X_valid). The predictions (y_pred) will be in the form of probability distributions across the classes for each pixel.\n",
        "\n",
        "- Convert Predictions to Class Labels: Use np.argmax to convert the predicted probability distributions to class labels. This will give the predicted class for each pixel.\n",
        "\n",
        "- Flatten the Masks: Flatten both the ground truth masks (y_valid) and the predicted masks (y_pred). Flattening converts the 2D masks into 1D arrays, making it easier to calculate metrics.\n",
        "\n",
        "- Convert Pixel Values to Binary: The evaluation metrics rely on generating a confusion matrix for a binary segmentation mask, comprising counts for true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) predictions. The values of all metrics discussed herein range from zero (indicating the poorest performance) to one (signifying the optimal performance). You need to convert the pixel values to binary (0 or 1). Use a threshold (e.g., 0.5) to decide the binary value for each pixel.\n",
        "\n",
        "- Calculate Metrics: Use the provided metric functions (precision_score_, recall_score_, accuracy, dice_coef, iou) to compute the metrics on the binary masks.\n",
        "        \n",
        "- Finally, print the computed metrics."
      ],
      "metadata": {
        "id": "BMtZ3pMjKahX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Case: U-Net with Transfer Learning"
      ],
      "metadata": {
        "id": "QX5iKJ5XOCLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Predict on the Validation Set\n",
        "# Use your trained model to make predictions on the validation set\n",
        "\n",
        "# Step 2: Convert Predictions to Class Labels\n",
        "# Convert predicted probabilities to class labels\n",
        "\n",
        "# Step 3: Flatten the Masks\n",
        "# Flatten the ground truth masks and predicted masks\n",
        "\n",
        "# Step 4: Convert Pixel Values to Binary\n",
        "# Use a threshold\n",
        "\n",
        "\n",
        "# Step 5: Calculate Metrics\n",
        "# Calculate each metric using the provided functions\n",
        "\n",
        "# Print the results"
      ],
      "metadata": {
        "id": "JoY8C42GtCdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Case: U-Net without Transfer Learning"
      ],
      "metadata": {
        "id": "ILB3c0tAOIhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Predict on the Validation Set\n",
        "# Use your trained model to make predictions on the validation set\n",
        "\n",
        "# Step 2: Convert Predictions to Class Labels\n",
        "# Convert predicted probabilities to class labels\n",
        "\n",
        "# Step 3: Flatten the Masks\n",
        "# Flatten the ground truth masks and predicted masks\n",
        "\n",
        "# Step 4: Convert Pixel Values to Binary\n",
        "# Use a threshold\n",
        "\n",
        "\n",
        "# Step 5: Calculate Metrics\n",
        "# Calculate each metric using the provided functions\n",
        "\n",
        "# Print the results"
      ],
      "metadata": {
        "id": "mY5Dd_a5OMiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}